[2025-04-03T09:51:44.956+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: full_pipeline_dag.clean_data_spark manual__2025-04-03T09:49:03.840756+00:00 [queued]>
[2025-04-03T09:51:45.035+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: full_pipeline_dag.clean_data_spark manual__2025-04-03T09:49:03.840756+00:00 [queued]>
[2025-04-03T09:51:45.036+0000] {taskinstance.py:2170} INFO - Starting attempt 3 of 3
[2025-04-03T09:51:45.116+0000] {taskinstance.py:2191} INFO - Executing <Task(BashOperator): clean_data_spark> on 2025-04-03 09:49:03.840756+00:00
[2025-04-03T09:51:45.120+0000] {standard_task_runner.py:60} INFO - Started process 166 to run task
[2025-04-03T09:51:45.123+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'full_pipeline_dag', 'clean_data_spark', 'manual__2025-04-03T09:49:03.840756+00:00', '--job-id', '155', '--raw', '--subdir', 'DAGS_FOLDER/full_pipeline_dag.py', '--cfg-path', '/tmp/tmpvhl6k_w_']
[2025-04-03T09:51:45.128+0000] {standard_task_runner.py:88} INFO - Job 155: Subtask clean_data_spark
[2025-04-03T09:51:45.235+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:194 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-04-03T09:51:45.285+0000] {task_command.py:423} INFO - Running <TaskInstance: full_pipeline_dag.clean_data_spark manual__2025-04-03T09:49:03.840756+00:00 [running]> on host f3e96b61807d
[2025-04-03T09:51:45.373+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='full_pipeline_dag' AIRFLOW_CTX_TASK_ID='clean_data_spark' AIRFLOW_CTX_EXECUTION_DATE='2025-04-03T09:49:03.840756+00:00' AIRFLOW_CTX_TRY_NUMBER='3' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-04-03T09:49:03.840756+00:00'
[2025-04-03T09:51:45.374+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-04-03T09:51:45.375+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', '\n        docker run --rm         --network shared-net         -v /opt/airflow/spark-app:/opt/spark-app         -v /opt/data:/opt/data         bitnami/spark:3.5         /opt/spark/bin/spark-submit /opt/spark-app/clean_yellow_tripdata.py\n        ']
[2025-04-03T09:51:45.380+0000] {subprocess.py:86} INFO - Output:
[2025-04-03T09:51:45.780+0000] {subprocess.py:93} INFO - [38;5;6mspark [38;5;5m09:51:45.77 [0m[38;5;2mINFO [0m ==>
[2025-04-03T09:51:45.782+0000] {subprocess.py:93} INFO - [38;5;6mspark [38;5;5m09:51:45.78 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2025-04-03T09:51:45.784+0000] {subprocess.py:93} INFO - [38;5;6mspark [38;5;5m09:51:45.78 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2025-04-03T09:51:45.785+0000] {subprocess.py:93} INFO - [38;5;6mspark [38;5;5m09:51:45.78 [0m[38;5;2mINFO [0m ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.
[2025-04-03T09:51:45.786+0000] {subprocess.py:93} INFO - [38;5;6mspark [38;5;5m09:51:45.78 [0m[38;5;2mINFO [0m ==>
[2025-04-03T09:51:45.794+0000] {subprocess.py:93} INFO - 
[2025-04-03T09:51:45.795+0000] {subprocess.py:93} INFO - /opt/bitnami/scripts/spark/entrypoint.sh: line 87: /opt/spark/bin/spark-submit: No such file or directory
[2025-04-03T09:51:46.103+0000] {subprocess.py:97} INFO - Command exited with return code 127
[2025-04-03T09:51:46.119+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 212, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 127.
[2025-04-03T09:51:46.124+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=full_pipeline_dag, task_id=clean_data_spark, execution_date=20250403T094903, start_date=20250403T095144, end_date=20250403T095146
[2025-04-03T09:51:46.164+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 155 for task clean_data_spark (Bash command failed. The command returned a non-zero exit code 127.; 166)
[2025-04-03T09:51:46.183+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2025-04-03T09:51:46.215+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check

[2025-04-03T09:55:32.468+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: full_pipeline_dag.clean_data_spark manual__2025-04-03T09:52:50.719852+00:00 [queued]>
[2025-04-03T09:55:32.485+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: full_pipeline_dag.clean_data_spark manual__2025-04-03T09:52:50.719852+00:00 [queued]>
[2025-04-03T09:55:32.486+0000] {taskinstance.py:2170} INFO - Starting attempt 3 of 3
[2025-04-03T09:55:32.568+0000] {taskinstance.py:2191} INFO - Executing <Task(BashOperator): clean_data_spark> on 2025-04-03 09:52:50.719852+00:00
[2025-04-03T09:55:32.571+0000] {standard_task_runner.py:60} INFO - Started process 311 to run task
[2025-04-03T09:55:32.575+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'full_pipeline_dag', 'clean_data_spark', 'manual__2025-04-03T09:52:50.719852+00:00', '--job-id', '160', '--raw', '--subdir', 'DAGS_FOLDER/full_pipeline_dag.py', '--cfg-path', '/tmp/tmps8fzv4ce']
[2025-04-03T09:55:32.578+0000] {standard_task_runner.py:88} INFO - Job 160: Subtask clean_data_spark
[2025-04-03T09:55:32.626+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:194 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-04-03T09:55:32.803+0000] {task_command.py:423} INFO - Running <TaskInstance: full_pipeline_dag.clean_data_spark manual__2025-04-03T09:52:50.719852+00:00 [running]> on host f3e96b61807d
[2025-04-03T09:55:32.955+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='full_pipeline_dag' AIRFLOW_CTX_TASK_ID='clean_data_spark' AIRFLOW_CTX_EXECUTION_DATE='2025-04-03T09:52:50.719852+00:00' AIRFLOW_CTX_TRY_NUMBER='3' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-04-03T09:52:50.719852+00:00'
[2025-04-03T09:55:32.957+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-04-03T09:55:32.957+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', '\n        docker run --rm         --network shared-net         -v /opt/airflow/spark-app:/opt/spark-app         -v /opt/data:/opt/data         bitnami/spark:3.5         /opt/bitnami/spark/bin/spark-submit /opt/spark-app/clean_yellow_tripdata.py\n        ']
[2025-04-03T09:55:32.962+0000] {subprocess.py:86} INFO - Output:
[2025-04-03T09:55:33.343+0000] {subprocess.py:93} INFO - [38;5;6mspark [38;5;5m09:55:33.33 [0m[38;5;2mINFO [0m ==>
[2025-04-03T09:55:33.344+0000] {subprocess.py:93} INFO - [38;5;6mspark [38;5;5m09:55:33.33 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2025-04-03T09:55:33.345+0000] {subprocess.py:93} INFO - [38;5;6mspark [38;5;5m09:55:33.33 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2025-04-03T09:55:33.346+0000] {subprocess.py:93} INFO - [38;5;6mspark [38;5;5m09:55:33.33 [0m[38;5;2mINFO [0m ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.
[2025-04-03T09:55:33.347+0000] {subprocess.py:93} INFO - [38;5;6mspark [38;5;5m09:55:33.33 [0m[38;5;2mINFO [0m ==>
[2025-04-03T09:55:33.347+0000] {subprocess.py:93} INFO - 
[2025-04-03T09:55:34.892+0000] {subprocess.py:93} INFO - python3: can't open file '/opt/spark-app/clean_yellow_tripdata.py': [Errno 2] No such file or directory
[2025-04-03T09:55:34.902+0000] {subprocess.py:93} INFO - 25/04/03 09:55:34 INFO ShutdownHookManager: Shutdown hook called
[2025-04-03T09:55:34.905+0000] {subprocess.py:93} INFO - 25/04/03 09:55:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-ef5b0529-2a5d-4772-964e-0fc21373b0b4
[2025-04-03T09:55:35.263+0000] {subprocess.py:97} INFO - Command exited with return code 2
[2025-04-03T09:55:35.278+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 212, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 2.
[2025-04-03T09:55:35.283+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=full_pipeline_dag, task_id=clean_data_spark, execution_date=20250403T095250, start_date=20250403T095532, end_date=20250403T095535
[2025-04-03T09:55:35.335+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 160 for task clean_data_spark (Bash command failed. The command returned a non-zero exit code 2.; 311)
[2025-04-03T09:55:35.361+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2025-04-03T09:55:35.397+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
